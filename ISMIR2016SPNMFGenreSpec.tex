% -----------------------------------------------
% Template for ISMIR Papers
% 2016 version, based on previous ISMIR templates

% Requirements :
% * 6+1 page length maximum
% * 2MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir,amsmath,cite}
\usepackage{graphicx}
\usepackage{color}
\usepackage[]{algorithm2e}
\usepackage{color}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage[justification=centering]{caption}
\usepackage{epstopdf}
\usepackage{float} 
\usepackage{amssymb}
\usepackage[]{algorithm2e}
\usepackage{enumitem} 
%\sloppy


% Title.
% ------
\title{Genre specific dictionaries for harmonic/percussive source separation}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
%\threeauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
\fourauthors
  {Cl\'{e}ment Laroche} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}


\begin{document}
%
\maketitle
%
\begin{abstract}

Supervised algorithms for audio source separation use apriori information like training data and dictionaries to achieve a decomposition. Audio signals are diverse and it is generally impossible to build prior information that is relevant on any audio signal. Most methods are tested on small databases that do not allow exhaustive comparison of the algorithms. When using a large database, the algorithms are not robust to the wide variety of audio signals and they do not provide satisfying results without user intervention. In this article, we focus on harmonic/percussive source separation and we propose to use musical genre information to guide the decomposition on a large database. Our method proves that the musical genre is a relevant feature as the drum dictionaries built using genre specific information perform better that an universal drum dictionary.  



\end{abstract}
%
\section{Introduction}\label{sec:introduction}


\emph{Source separation} is the field of ​​research that seeks to separate the components of an audio signal composing a record. Such separation has many applications in music: up-mixing \cite{fitzgerald2011upmixing} (spatialization of the sources) or automatic transcription \cite{vincent2010adaptive} (it is easier to work on single source). The task is difficult due to the complexity and the variability of the music mixtures. Most datasets used for Blind Audio Source Separation (BASS) research are small in size and they do not allow for a thorough comparison of the source separation algorithms. Using a larger database is crucial to benchmark the different algorithms in order to obtain a true evaluation rather than particular case results. 

The large variety of audio signals can be classified into different musical genres \cite{tzanetakis2002musical}. Genres are labels created and used by humans for categorizing and describing music. They have no strict definitions and boundaries but particular genre share certain characteristics typically related to instrumentation, rhythmic structure, and pitch content of the music. This resemblance between two pieces of music have been used to improve chord transcription \cite{ni2012using,lee2008acoustic} or downbeat detection \cite{hockman2012one} algorithms. Genre information is obtained using annotated labels. When the genre information is not available, it can be retrived using automatic genre classification \cite{tzanetakis2002musical,mckay2006musical}.

In the context of BASS, Non-negative Matrix Factorization (NMF) is a widely used method. The goal of NMF is to approximate a data matrix $V \in \mathbb{R}_{+}^{n \times m} $ as 
\begin{equation}\label{modelNMF}
V \approx \tilde{V} = WH
\end{equation}
with $W \in \mathbb{R}_{+}^{n \times k}$, $H \in \mathbb{R}_{+}^{k \times m}$ and where $k$ is the rank of factorization \cite{lee99}. In audio signal processing, the input data is usually a Time-Frequency (TF) representation such as a short time Fourier transform (STFT) or a constant-Q transform spectrogram. Blind source separation is a difficult problem and the plain NMF decomposition does not provide satisfying results. To obtain a satisfying decomposition, it is necessary to exploit various features that make each sources distinguishable from one another. 
Supervised algorithms in the NMF framework exploit training data or prior information in order to guide the decomposition process. For example, information from the scores or from midi signals \cite{EwertM12} can be used to initialize the learning process. The downside of these approachies is that they require well organized prior information that is not always available. Another supervised method consists in performing prior training on specific databases. A dictionary matrix $W_{train}$ is learned from a big database in order to separate an instrument \cite{jaureguiberry2011adaptation,wudrum}. These methods require minimum tuning from the user. However, the dictionaries must match the target instruments for satisfying performances. 


In this paper, we focus on the task of harmonic/percussive source separation (HPSS) using the method developed in \cite{laroche2015structured}. 
Harmonic/percussive source separation has numerous applications as a preprocessing step. For example most multi-pitch estimation models~\cite{klapuri2008multipitch}, instruments recognition~\cite{eronen2000musical} and melody extraction~\cite{salamon2012melody} algorithms are much more efficient on harmonic data only. Similarly, beat tracking~\cite{ellis2007beat} and drum transcription algorithms~\cite{paulus2005drum} are more accurate if the harmonic instruments are not part of the signal. Finally, using the HPSS algorithm~\cite{fitzgerald2010harmonic} as a preprocessing step increases the performance for singing pitch extraction and voice separation~\cite{hsu2012tandem}. 
We adapt the algorithm to use a trained drum dictionary to extract the percussive instruments. This method is detailled \cite{larocheJournal}. 
The problem of using a fixed dictionary matrix is that within a database, the same instrument can sound differently depending on the recording conditions and post processing treatments. In order to represent correctly one instrument, one can decide to learn a dictionary on a large database but take the risk of over-fitting the data. In order to avoid this problem and to build effective dictionaries, we decided to use genre specific training data. As songs with identical style share similar features, genre specific information can provide an insight on the structure of the audio signal.
The main contribution of this article is that we developed a genre specific method to build NMF drum dictionaries that obtain consistent and robust results on a HPSS task. The genre specific dictionaries are able to improve the separation score compared to a universal dictionary. 


Section \ref{defgenre} defines the context of our work, and Section \ref{sec:SPNMF} presents the algorithm used while Section \ref{ConstrucDict} describes the construction of dictionaries specific. Finally Section \ref{sec:results} details the results of the HPSS on $65$ audio files and \ref{sec:conclusion} concludes our work. 



\section{Toward genre specific information}\label{defgenre}

\subsection{Genre information}

Musical genre is one of the most prominent high level music descriptor. Electronic Music Distribution have become more popular in recent years and music catalogues never stop to increase (the biggest online services propose around 1 million tracks); in that context, associating a genre to a musical piece is crucial to help users finding what they are looking for. If an explicit definitions of musical genres is still out of reach \cite{aucouturier2003representing}, musical genre classification can be performed automatically using different set of features\cite{li2003comparative}. 

Source separation have been used extensively in order to help the classification process \cite{rump2010autoregressive,lampropoulos2005musical}. However, genre information have not been exploited to guide the decomposition process. 


\subsection{Methods for dictionary learning}


Audio data is largely redundant in two main aspects: it often contains multiple correlated versions of the same physical event (note, drum hits...) and each version is usually densely sampled \cite{tovsic2011dictionary}. The relevant information is generally of much reduced dimensionality compared to the original data sets hence the idea to exploit this redundancy to reduce the amount of information necessary for the representation of a musical signal. Many rank reduction methods, such as Single Value Decomposition (K-SVD) \cite{aharon2006img}, Vector Quantization (VQ) \cite{gersho2012vector}, Principal Component Analysis (PCA) \cite{huang2012singing}, sparse coding ~\cite{abdallah2006unsupervised}  or Non negative matrix factorization (NMF) \cite{Smaragdis2003} are based on the principle that our observations can be described by a sparse subset of atoms taken from a redundant representation hence the idea to use these methods to create genre specific drum dictionaries.

Building dictionary using K-SVD is a successful method for image processing \cite{zhang2010discriminative}, however, this method does not scale well to process large audio signals and a genre specific dictionary cannot be considered in this framework.

VQ has been mainly used for audio compression, sparse coding has been used among other things for source separation \cite{plumbley2010sparse}, and PCA has been used for voice extraction. However these methods do not have been used yet has a pre-processing step to build a dictionary.

Finally, in the NMF framework, some work has been done to perform a decomposition with dictionaries. In \cite{hennequin2011score}, the dictionary is built using a physical model of the instrument. This method is not adapted to build genre specific data. Another method using NMF to build a dictionary is to compute a NMF decomposition on a large training set specific to the target source. After the optimization process of the NMF, the $W$ matrix from the decomposition is used as the dictionary matrix $W_{train}$. This method is difficult to use on pitched instruments (i.e., Harmonic instruments) and the dictionary needs to be adaptated \cite{jaureguiberry2011adaptation}, however, it provided good results for HPSS \cite{larocheJournal}. The rank of the factorization determines the final size of the dictionary and it can be chosen small enough to obtain a strong compression of the original data. This property motivated us to build genre specific data using NMF. The selection of the rank of factorization will be discussed in Section \ref{optimalsize}.  

\subsection{Genre information for HPSS}

Current state-of-the-art unsupervised methods for HPSS such as \cite{ono2008separation} and \cite{canadas2014percussive} cannot be easily adapted to use genre information. However, supervised algorithms however can use genre specific dictionaries. In~\cite{kim2011nonnegative} the drum source separation is done using a Non-Negative Matrix Partial Co-Factorization (NMPCF). The spectrogram of the signal and the drum-only data (obtained from prior learning) are simultaneously decomposed in order to determine common basis vectors that capture the spectral and temporal characteristics of the drum sources. The percussive part of the decomposition is constrained while the harmonic part is completely unconstrained. As a result, it tends to decompose a lot of information from the signal and the decomposition is not satisfactory (i.e., the harmonic part contains some percussive instruments). The other major problem of this method is that it does not scale when the training data are large and the computation time is significantly larger compared to other methods.

Finally the method first introduced by \cite{laroche2015structured} and detailed in \cite{larocheJournal} is a good candidate to test the genre specific dictionaries as they are easily integrated to the method and do not increase the computation time. The method is detailed in Section \ref{sec:SPNMF}.



\section{Structured projective NMF (SPNMF)}
\label{sec:SPNMF}

In this section we present our algorithm for harmonic/percussive source separation.


\subsection{Principle of the SPNMF}

Using a similar model as in our preliminary work~\cite{laroche2015structured}, let $V$ be the magnitude spectrogram of the input data. The model is then given by
\begin{equation} \label{Cfunction}
V \approx \tilde{V}= V_H + V_{P},
\end{equation}
with $V_P$ the spectrogram of the percussive part and $V_H$ the spectrogram of the harmonic part. $V_H$ is approximated by the projective NMF decomposition \cite{yuanOja2005} while $V_P$ is decomposed by NMF components which leads to:
\begin{equation}
V \approx \tilde{V}= W_{H}W_{H}^{T}V + W_{P} H_{P}.
\end{equation}
The data matrix is approximated by an almost orthogonal sparse part that codes the harmonic instruments $V_H = W_HW_H^T V$ and a non constrained NMF part that codes the percussive instruments $V_P = W_PH_P$. We use here a fixed genre specific drum dictionary $W_P$ in the percussive part of the SPNMF as a fully unsupervised SPNMF model does not allow for a satisfying harmonic/percussive source separation~\cite{laroche2015structured}.



\subsection{Algorithm Optimization}

In order to obtain such a decomposition, we can use a measure of fit $D(x|y)$ between the data matrix $V$ and the estimated matrix $\tilde{V}$. $D(x|y)$ is a scalar cost function and in this article, we use the Itakura Saito (IS) divergence.

The SPNMF model gives the cost function : 
\begin{equation}\label{InitCost}
\min_{W_H,W_P,H_P \geq 0} D(V|W_{H}W_{H}^{T}V + W_{P} H_{P})  
\end{equation}

A solution to this problem can be obtained by iterative multiplicative update rules following the same strategy as in~\cite{yuanOja2005,Lee01algorithmsfor} which consists in splitting the gradient with respect to (wrt) one variable (here $W_H$ for exemple) $\nabla_{W_H} D(V|\tilde{V})$ in its positive $[\nabla_{W_H} D(V|\tilde{V})]^{+}$ and negative parts $[\nabla_{W_H} D(V|\tilde{V})]^{-}$, if $\otimes$ is the Hadamard product or element-wise product. Using formula from \ref{ISdisteq} the algorithm \ref{AlgoDictionary} gives the SPNMF optimization process.
 
\begin{algorithm}[h]
 Input: $V \in \mathbb{R}_{+}^{m \times n} $
 Output: $W \in \mathbb{R}_{+}^{m \times k}$, $W_{train} \in \mathbb{R}_+^{m \times e}$ and $H \in \mathbb{R}_{+}^{e \times n}$
 Initialization\;
 \While{$i \leq$ number of iterations}{
	$H_{P} \leftarrow H_{P} \otimes \frac{ [\nabla_{H_P} D(V|\tilde{V})]^{-} }{[\nabla_{H_P} D(V|\tilde{V})]^{+}}$
	

	$W_{H} \leftarrow W_{H} \otimes \frac{ [\nabla_{W_H} D(V|\tilde{V})]^{-} }{[\nabla_{W_H} D(V|\tilde{V})]^{+}}$
	 \vspace{0.2cm}
	 	
	$i=i+1$ 
 }
 $ X_P = W_{train}H_P $ and
 $ X_H = W_HW_H^TV $ 
  
\vspace{0.2cm}
 \caption{SPNMF with the drum dictionary matrix.}\label{AlgoDictionary}
\end{algorithm}




 
\subsection{Signal reconstruction}

The percussive signal $x_p(t)$ is synthesized using the magnitude percussive spectrogram $X_P = W_PH_P$. To reconstruct the phase of the percussive part, we use a Wiener filter~\cite{liutkus2015generalized} to create a percussive mask as:
\begin{equation}
\mathcal{M}_P = \frac{X_P^2}{X_M^2 + X_P^2}.
\end{equation} 
To retrieve the percussive signal as, 
\begin{equation}
x_p(t) = InverseSTFT(\mathcal{M}_P \otimes X).
\end{equation}
Where $X$ is the complex spectrogram of the mixture.
We use a similar process for the harmonic part.
%, we obtain:
%\begin{equation}\label{percuweiner}
%\mathcal{M}_H = \frac{X_H^2}{X_M^2 + X_P^2},
%\end{equation}
%and:
%\begin{equation}
%x_h(t) = InverseSTFT(\mathcal{M}_H \otimes X).
%\end{equation}
%



\section{Construction of the dictionary}\label{ConstrucDict}

In this Section, we present in Section \ref{optimalsize} the test conducted on the SiSEC $2010$ database~\cite{SiSec10} in order to find the optimal size to build the genre specific dictionaries. In Section \ref{database} we describe the training and the evaluation database. Finally, in Section \ref{genrespecdict}, we detail the protocol to build the genre specific dictionaries. 

\subsection{Optimal size for the dictionary}\label{optimalsize}

The first step to build a NMF drum dictionary is to select the rank of factorization. We run the optimization tests on the public SiSec database from~\cite{SiSec10} to avoid overtraining. The dataset is composed of $4$ polyphonic real-world music excerpts and each music signal contains percussive, harmonic instruments and vocals. The duration of the four recording is ranging from $14$ to $24$~s. Following the same protocol as in~\cite{canadas2014percussive}, we will not consider the vocal part and we will build the mixture signals from the percussive and harmonic instruments only. The signals are sampled at $44.1$ kHz. We compute the STFT with a $2048$ sample long Hann window with a $50\%$ overlap. Furtheremore, the rank of the factorization of the harmonic part for the SPNMF algortihm is $k=100$ as in \cite{larocheJournal}.

The drum signal used for the training comes from the database ENST-Drums \cite{gillet2006enst} and the signal is around $10$ min long. We used $30$ files were the drummer is playing a \emph{drum phrase}. We compute an NMF decomposition with different rank of factorization ($k=12$, $k=50$, $k=100$, $k=200$, $k=300$, $k=500$, $k=1000$ and $k=2000$) on the drum signal alone to obtain $8$ drum dictionaries.

The dictionaries are then used to perform a HPSS on the four songs of the SiSEC database using the SPNMF algorithm. The results are compared by means of the Signal-to-Distortion Ratio (SDR), the Signal-to-Interference Ratio (SIR) and the Signal-to-Artifact Ratio (SAR) of each of the separated sources using the BSS Eval toolbox provided in \cite{bsseval}.

\begin{figure}[h]

  \centering 
  \includegraphics[width=9cm]{figs/AllDictSizeISMIR.eps}
%  \vspace{2.0cm}
  \caption{\label{dictsize}Influence of k on the S(D/I/A)R on the SiSEC database.}
  
\end{figure}

The results on figure \ref{dictsize} show that the optimal value for the SDR and SIR is reached for $k=100$, then the SDR decreases for $k\geqslant 200$. The high value of SAR (for $k\geqslant 500$) are explained by the fact that the separation process is not satisfying. The harmonic signal provided by the algorithm is composed of most of the original signal therefore the SAR is very high but the decomposition quality is poor. 

The optimal dictionary size for the SPNMF algorithm is $k=100$ so we will use it in the next Section. $k=50$ and $k=200$ does not lead to significantly different. The method is robust and the to the rank of factorization. 

\subsection{Training and evaluation database}\label{database}

The dataset Medley-dB \cite{bittner2014medleydb} is used for our tests. It is composed of polyphonic real-world music excerpts. It has $122$ music signals and $85$ of them contain percussive instruments, harmonic instruments and vocals. The signals that do not contain a percussive part are not part of the evaluation. The genres are, \emph{Classical} ($8$ songs), \emph{Singer/Songwriter} ($17$ songs), \emph{Pop} ($10$ songs), \emph{Rock} ($20$ songs), \emph{Jazz} ($11$ songs), \emph{Electronic/Fusion} ($13$ songs) and \emph{World/Folk} ($6$ songs). Because the notion of genre is quite subjective (see Section \ref{defgenre}), the medley-dB database uses general genre labels. These labels should not be considered to be precise genre labels. There are many instances where a song could have fallen in multiple genres, and the choices were made so that each genre would be as acoustically homogeneous as possible. As we are only working with the instrumental part of the song, the \emph{Pop} label (for example) are similar to the \emph{Singer/Songwriter}.

The training dataset is built using $3$ song of each genre. The songs used for the training part are used in of the evaluation. To compare the genre specific dictionaries, we build a non specific/universal dictionary using half of one song of each genre. Finally, a last dictionary is built using around $10$min of pure drum signals from the ENST-Drums database as in Section \ref{optimalsize}. The files from medley-dB selected for the training are given in Table \ref{trainingdata}. 


\begin{table} 
	\centering 
	\small
   \begin{tabular}{|l|l|}
   \hline
   Genre & Artist Song \\
   	\hline   
Classical  & JoelHelander Definition \\
 & MatthewEntwistle AnEveningWithOliver \\
 & MusicDelta Beethoven \\
\hline
Electronic/Fusion & EthanHein 1930sSynthAndUprightBass \\
 & TablaBreakbeatScience Animoog \\
 & TablaBreakbeatScience Scorpio \\
\hline
Jazz & CroqueMadame Oil \\
 & MusicDelta BebopJazz \\
 & MusicDelta ModalJazz \\
\hline
Pop &  DreamersOfTheGhetto HeavyLove \\
 & NightPanther Fire \\
 & StrandOfOaks Spacestation \\
\hline
Rock & BigTroubles Phantom \\
 & Meaxic TakeAStep \\
 & PurlingHiss Lolita \\
\hline
Singer/Songwriter	& AimeeNorwich Child \\
 & ClaraBerryAndWooldog Boys \\
 & InvisibleFamiliars DisturbingWildlife \\
\hline
World/Folk & AimeeNorwich Flying \\
 &KarimDouaidy Hopscotch \\
 & MusicDelta ChineseYaoZu\\
\hline
Non specific & JoelHelander Definition \\
 & TablaBreakbeatScience Animoog \\
 & MusicDelta BebopJazz \\
 & DreamersOfTheGhetto HeavyLove \\
 &  BigTroubles Phantom \\
 & AimeeNorwich Flying \\
 &  MusicDelta ChineseYaoZu\\
 \hline

  
\end{tabular} 
\caption{\label{trainingdata} Song selected for the training database.}

\end{table}





\subsection{Genre specific dictionaries}\label{genrespecdict}

The NMF model is given by \eqref{modelNMF}. If $V$ is the power spectrum of a drum signal, The matrix $W$ is a {\em dictionary} or a set of {\em patterns} that codes the frequency information of the drum. Here we build genre specific drum dictionaries using the medley-dB database. %Using dictionaries specific to the genre of music allows to have dictionaries that a more specific to the signal to decompose.

%Table \ref{lengthDict} gives us the length and the energy of the training signal for each genre.
%\emph{Classical} and \emph{Electronic/Fusion} songs are composed of songs where the drum is only playing for a few moments and even though the signals are longer than the other. 
With the results from Section \ref{optimalsize} the dictionaries are built as follows. For every genre specific subset of the training database, we perform a NMF on the drum signals with $k=100$. Then the $W$ matrices of the NMF are used in the SPNMF algorithm as the matrix $W_P$ (see algorithm \ref{AlgoDictionary}).

%\begin{table}
%   
%	\centering 
%   \begin{tabular}{|l|l|}
%\hline   
%Genre & Length(min) \\
%\hline
%Classical  & 22.06  \\
%\hline
%Electronic/Fusion & 18.66 \\
%\hline
%Jazz & 10.96 \\
%\hline
%Pop &  12.53 \\
%\hline
%Rock & 11.43 \\
%\hline
%Singer/Songwriter & 9.36 \\
%\hline
%World/Folk & 9.53 \\
%\hline
%Non Specific (Mix) & 11.03  \\
%\hline
%  
%\end{tabular} 
%\caption{\label{lengthDict} Length of the genre specific database.}
%\end{table}


\section{Results}\label{sec:results}

In this Section, we present the results of the SPNMF with the genre specific dictionaries on the evaluation database from Medley-dB.

\subsection{Comparison of the dictionaries}

In this section we present the results of the SPNMF algorithm with the genre specific dictionaries on the $64$ remaining song from test database Medley-dB. We perform an HPSS on the audio files using the SPNMF with the $9$ dictionaries created in Section \ref{genrespecdict}. The results on each of the songs are then sorted by genres and the average results are displayed using box-plots. Each box-plot is made up of a central line indicating the median of the data, upper and lower box edges indicating the $1^{st}$ and $3^{rd}$ quartiles while the whiskers indicate the minimum and maximum values. 


The Figures \ref{sdrpop}, \ref{sirpop} and \ref{sarpop} show the SDR, SAR and SIR results for all the dictionaries on the \emph{Pop} subsection. It gives us an overall idea of the performance of the dictionaries on the same database. The \emph{Pop} dictionary leads to the highest SDR and SIR. The non specific dictionaries are not performing as well as the \emph{Pop} dictionary. On this database, the genre specific data gives relevant information to the algorithm. As stated in Section \ref{database}, some genres are similar to other. This explains why the \emph{Rock} and the \emph{Singer} dictionaries are providing good results too. 
An interesting result is that compared to the non specific dictionary, the \emph{Pop} dictionary has a lower variance. Genre information allows for a higher robustness to the variety of the songs within the same genre.  

The dictionary built on the ENST-drums is giving very similar results to the universal dictionary built on the Medley-dB database. For the sake of concision we only display the results using the universal dictionary from Medley-dB. 


\begin{figure}[h]

  \centering 
  \includegraphics[width=9cm]{figs/SDRismirpop.eps}
%  \vspace{2.0cm}
  \caption{\label{sdrpop} Percussive (left bar)/Harmonic (right bar) SDR results on the \emph{Pop} sub-database using the SPNMF with the 9 dictionaries.}
  
\end{figure}\begin{figure}[h]

  \centering 
  \includegraphics[width=9cm]{figs/SIRismirpop.eps}
%  \vspace{2.0cm}
  \caption{\label{sirpop} Percussive (left bar)/Harmonic (right bar) SIR results on the \emph{Pop} sub-database using the SPNMF with the 9 dictionaries.}
  
\end{figure}\begin{figure}[h]

  \centering 
  \includegraphics[width=9cm]{figs/SARismirpop.eps}
%  \vspace{2.0cm}
  \caption{\label{sarpop} Percussive (left bar)/Harmonic (right bar) SAR results on the \emph{Pop} sub-database using the SPNMF with the 9 dictionaries.}
  
\end{figure}

%\begin{table*}
%   
%   \begin{tabular}{|c|c|c|c|c|c|c|c|}
%\hline   
%Genre & Classical & Electronic/Fusion & Jazz & Pop & Rock & Singer/Songwriter & World/Folk \\
%\hline
%Genre specific Results(dB)  & & & & & & & \\
%SDR & 2.95     & \bf{0.54}& 5.94     & \bf{3.76}&\bf{0.97} &\bf{4.69} &\bf{1.83} \\
%SIR & \bf{9.41}& 8.51     & \bf{11.1}& \bf{9.77}&\bf{12.0} & 9.80     &\bf{8.33} \\
%SAR & 12.0     & 11.9     & \bf{15.6}& 13.9     &\bf{19.1} &\bf{17.2} &\bf{18.6} \\
%\hline
%Non specific (dB) & & & & & & &\\
%SDR & \bf{3.00}& 0.50     & \bf{6.02}& 3.40     & -0.11    &  3.71    & 0.66 \\
%SIR & 9.25     & \bf{9.18}& 10.5     & 8.72     & 10.6     &\bf{10.3} & 7.53 \\
%SAR & \bf{17.6}& \bf{14.1}& 15.4     & \bf{14.5}& 15.4     & 16.9     & 16.4 \\
%\hline
%  \end{tabular} 
%\caption{\label{specresults} Mean SDR, SIR and SAR results on the Medley-dB database.}
%\end{table*}

\begin{table*}
   
   \begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline   
Genre & Classical & Electronic/Fusion & Jazz & Pop & Rock & Singer/Songwriter & World/Folk \\
\hline
Percussive separation & & & & & & & \\
\hline
Genre specific (dB)  & & & & & & & \\
SDR & -1.64     & -0.63     &\bf{0.36} & \bf{2.45}&\bf{-0.17}& \bf{0.64} & \bf{0.42} \\
SIR & 8.21      & 15.17     &\bf{9.60} & 12.34    &\bf{19.79}& 11.45     & \bf{6.08} \\
SAR & 5.88      & 0.31      &\bf{2.08} & \bf{3.36}& 0.31     & \bf{4.46} & \bf{16.29} \\
\hline
Non specific (dB) & & & & & & &\\
SDR & \bf{-0.04}& \bf{-0.25}& -0.68    & 2.01     & -2.15    & -0.01     & -3.57 \\
SIR & \bf{11.3} & \bf{17.01}& 9.57     &\bf{12.60}& 18.30    & \bf{13.04}& 2.82 \\
SAR & \bf{8.07} & \bf{0.39} & 0.87     & 2.74     &\bf{2.34} & 1.83      & 12.08 \\
\hline   
Harmonic Separation &  & & & & & & \\  
\hline
Genre specific (dB)  & & & & & & & \\
SDR & \bf{7.49}    & \bf{1.63}    & \bf{13.05} & \bf{5.06}  & \bf{2.14}& 7.20      &\bf{4.86} \\
SIR & \bf{10.60}   & \bf{1.84}    & \bf{13.27} & \bf{5.02 } & 2.19     & \bf{11.45}& \bf{13.50} \\
SAR & 18.19       & 23.48       & 28.50      & 24.48      &\bf{35.97}& 28.48     & \bf{22.65} \\
\hline
Non specific (dB) & & & & & & &\\
SDR & 6.04        & 1.33        & 12.71     & 4.78      & 1.92      & \bf{7.46}      & 4.64\\
SIR & 7.14        & 1.36        & 12.82     & 4.85      & \bf{2.86} & 7.50      & 13.27 \\
SAR & \bf{27.21}& \bf{27.72}   & \bf{29.87} & \bf{26.17}& 34.25     &\bf{31.87} & 21.64\\
\hline
  \end{tabular} 
\caption{\label{specresults} Mean harmonic SDR, SIR and SAR results on the Medley-dB database.}
\end{table*}



On Table \ref{specresults}, we display the mean separation score for all the genre specific dictionaries compared to the non specific dictionary. The genre specific dictionaries outperform the universal dictionary by a considerable margin on $5$ of the $7$ genres. The results are discussed in the next Section.


\subsection{Discussion}


On the database \emph{Singer/Songwriter}, \emph{Pop}, \emph{Rock}, \emph{Jazz} and \emph{World/Folk}, the genre specific dictionaries outperform the universal dictionary. 

The information from the music of the same genre is not altered by the NMF compression and the drum templates are closer to the target drum.  
The databases \emph{Classical} and \emph{Electronic/Fusion} are composed of songs where the drum is only playing for a few moments. Similarly on some songs of the \emph{Electronic/Fusion} database, the electronic drum reproduces the same pattern during the whole song making the drum part very redundant thus the drum dictionary does not contain a sufficient amount of information to outperform the universal dictionary. Because of these two factors, the genre specific dictionaries are not performing correctly.

Overall the harmonic separation is giving much better results than the percussive extraction. The fixed dictionaries are creating artefact as the percussive templates do not correspond exactly to the target drum signal. A possible way to alleviate this problem is to adapt the dictionaries but this requires the use of hyper parameters and that is not the philosophy of this work \cite{laroche2015structured}.



\section{Conclusion}\label{sec:conclusion}

Using genre specific information in order to build more relevant drum dictionaries is a powerful method to improve the HPSS. The dictionaries still have an imprint of the genre after the NMF decomposition and the additional information is properly used by the SPNMF to improve the source separation quality. This is a first step in order to produce dictionaries capable of extracting a wide variety of audio signal. 

Future work will be dedicated into building a blind method to select the genre specific dictionary in order to perform the same technique on database where the genre information is not available. 



\section{SPNMF with the IS divergence}\label{ISdisteq}
 
The Itakura Saito divergence gives us the problem,
$$\min_{W_1,W_2,H_2 \geq 0} \frac{V}{\tilde{V}} - log(\frac{V}{\tilde{V}}) +1.$$

The gradient wrt $W_1$ gives
$$[\nabla_{W_1} D(V|\tilde{V})]_{i,j}^{-} = (ZV^TW_1)_{i,j} + (VZ^TW_1)_{i,j},$$
with $Z_{i,j} = (\frac{V}{W_1W_1^TV + W_2H_2})_{i,j}$. 
The positive part of the gradient is
$$[\nabla_{W_1} D(V|\tilde{V})]_{i,j}^{+} = (\phi V^TW_1)_{i,j} + (V \phi^T W_1)_{i,j},$$
with $$ \phi_{i,j} = (\frac{I}{W_1W_1^TV + W_2H_2})_{i,j}.$$ and $I \in \mathbb{R}^{f \times t} ; \forall i,j \quad I_{i,j}=1$.


Similarly, the gradient wrt $W_2$ gives
$$ [\nabla_{W_2} D(V|\tilde{V})]^{-} = VH_2^T $$
and
$$ [\nabla_{W_2} D(V|\tilde{V})]^{+} = W_1W_1^TVH_2^T + W_2H_2H_2^T.$$

Finally, the gradient wrt $H_2$ gives
$$ [\nabla_{H_2} D(V|\tilde{V})]^{-} = W_2^TV  $$
and
$$ [\nabla_{H_2} D(V|\tilde{V})]^{+} = 2W_2^TW_1W_1^TV + W_2^TW_2H_2. $$




% For bibtex users:
\bibliography{reference}



\end{document}
