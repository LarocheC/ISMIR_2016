% -----------------------------------------------
% Template for ISMIR Papers
% 2016 version, based on previous ISMIR templates

% Requirements :
% * 6+1 page length maximum
% * 2MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir,amsmath,cite}
\usepackage{graphicx}
\usepackage{color}
\usepackage[]{algorithm2e}
\usepackage{color}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage[justification=centering]{caption}
\usepackage{epstopdf}
\usepackage{float} 

\usepackage{amssymb}
\usepackage[]{algorithm2e}
\usepackage{enumitem} 
\sloppy


% Title.
% ------
\title{Genre specific dictionaries for harmonic/percussive source separation}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
%\threeauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
\fourauthors
  {Cl\'{e}ment Laroche} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}


\begin{document}
%
\maketitle
%
\begin{abstract}

Supervised algorithms for audio source separation use apriori information like training data and dictionary to acheive a decomposition. Audio signal are diverse and it is generaly impossible to build prior information that is relevant on any audio signal. Most of the method for are tested on small databases that do not allow exhaustive comparison of the algorithm. The problem of using a large database is that the algorithms are not robust to the wide variety of audio signal and they do not provide satisfying results without user intervention. In this article, we concentrate on the case of harmonic/percussive source separation and we propose to use the musical genre information to perform a test on a large database. Our method proves that the musical genre is a relevant feature as the dictionary built using genre specific information perform better that an universal dictionary built on all genre.  



\end{abstract}
%
\section{Introduction}\label{sec:introduction}


\emph{Source separation} is field of ​​research that seeks to separate the components of an audio signal present in a record. Such separation has many applications in music : noise suppression \cite{boll1979suppression} (if a source is a noise) , up-mixing \cite{fitzgerald2011upmixing}(spatialization of the sources) or automatic transcription \cite{Bertin07} (it is easier to operate on single source). The task is difficult due to the complexity and the variability of the music mixtures. Most datasets used for Blind Source Separation (BSS) research are small in size and they do not allow for a thorough comparison of the algorithms. Using a larger database is crucial to benchmark the separation algorithms in order to obtain a true evaluation rather than particular case results. 

The large variety of audio signal can be classified into different musical genres \cite{tzanetakis2002musical}. Genres are labels created and used by humans for categorizing and describing music. They have no strict definitions and boundaries but particular genre share certain characteristics typically related to the instrumentation, rhythmic structure, and pitch content of the music. This resemblance between two pieces of music have been used to perform chord transcription \cite{ni2012using,lee2008acoustic} or for downbeat detection \cite{hockman2012one}. Finally when the genre information is not available, it is possible to perform automatic genre classification \cite{li2003comparative}.

In the context of BSS, Non-negative Matrix Factorization (NMF) is a widely used method for source separation. The goal of NMF is to approximate a data matrix $V \in \mathbb{R}_{+}^{n \times m} $ as $V \approx \tilde{V} = WH$ with $W \in \mathbb{R}_{+}^{n \times k}$, $H \in \mathbb{R}_{+}^{k \times m}$ and where $k$ is the rank of factorization \cite{lee99}. In audio signal processing, the input data is usually a Time-Frequency (TF) representation such as a short time Fourier transform (STFT) or a constant-Q transform spectrogram. Blind source separation is a difficult problem and the plain NMF decomposition does not provide satisfying results. To perform a satisfying decomposition, it is necessary to exploit various features that make each sources distinguishable from one another. 
Supervised algorithms in the NMF framework exploit training data or prior information in order to guide the decomposition process. For example information from the scores or from midi signals \cite{EwertM12} can be used to initialize the learning process. The downside of this approach is that it requires well organized prior information that is not always available. Another supervised method consists in performing prior training on specific databases. A dictionary matrix $W_{train}$ is learned from a big database in order to separate an instrument \cite{jaureguiberry2011adaptation,wudrum}. A common method to build a dictionary is to perform a NMF decomposition on a large training set. After the convergence, the $W$ matrix from the decomposition is used as the dictionary matrix $W_{train}$ in the separation \cite{jaureguiberry2011adaptation}. Another method is detailed in \cite{wudrum}, a dictionary matrix is created by extracting template spectra from isolated drum samples. The dictionary is then used in a NMF decomposition to perform drum transcription. This method requires minimum tuning from the user. However, the dictionary should match the target instrument for satisfying performances. 

In this paper, we focus on the task of harmonic/percussive source separation (HPSS) using the method developed in \cite{laroche2015structured}. We adapt the method to use a drum dictionary to extract the percussive instruments. This method is explained in detail in the preprint. 
The problem of using a fixed dictionary matrices is that within a database, the same instrument can sound differently depending on the recording conditions and post processing treatments. In order to represent correctly one instrument, ones can decide to learn a dictionary on a large database. However, the problem of over-fitting the data exist. In order to overcome this problem and to build effective dictionaries we decided to use genre specific training data. As they share similar features, genre specific information can provide an insight on the structure of the audio signal.
The main contribution of this article is that we developed a genre specific method to build a drum NMF dictionary that obtains consistent and robust results on a HPSS task. The genre specific dictionary are able to improve the separation score compared to a universal dictionary. 
 



\section{Genre specific information}\label{defgenre}



Musical genre is probably the most popular high level music descriptor. Electronic Music Distribution have become more popular in recent years and music catalogues never stop to increase (the biggest online services propose around 1 million tracks); in that context, associating a genre to a musical piece is crucial to help users finding what they are looking for. If an explicit definitions of musical genres is still out of reach \cite{aucouturier2003representing}, musical genre classification can be performed automatically using different set of features\cite{tzanetakis2002musical,mckay2006musical}. 

Source separation have been used extensively in order to help the classification process \cite{rump2010autoregressive,lampropoulos2005musical}. However, using genre information have not been exploited to guide the decomposition process. 




\section{Structured projective NMF (SPNMF)}
\label{sec:SPNMF}

In this section we present our semi-supervised algorithm for harmonic/percussive source separation.

\subsection{Presentation of the orthogonal and projective NMF}\label{subsec:PNMF}


The aim of PNMF is to find a non negative projection matrix $P \in \mathbb{R}_{+}^{n \times n}$ such that $V \approx \tilde{V} = PV$. In~\cite{yuanOja2005} Yuan \& al. propose to seek $P$ as an approximative projection matrix under the form $P = WW^{T}$ with $W \in \mathbb{R}_{+}^{n \times k}$ with $ k \leqslant n $. The PNMF problem reads : 
\begin{equation}\label{EqPnmf}
\min_{W \geqslant 0} ||V - WW^{T}V||^2 
\end{equation}

PNMF is similar to the NMF problem and can be simply obtained by replacing the activation matrix $H$ by $W^TV$. It is shown in~\cite{YangOja10} that the PNMF gives a much sparser decomposition than NMF.

Another very similar approach is the ONMF~\cite{choi}. It consists in solving the following problem: 
\begin{align}
\min_{W \geqslant 0, H \geqslant0} ||V - WH||^2 \quad   \text{s.t}.\quad W^{T}W=I_{k} 
\end{align}%
In this method, orthogonality between nonnegative basis functions is enforced during the optimization process. In theory, it seems that PNMF and ONMF lead to similar decompositions, as the $W$ matrix estimated by PNMF is almost orthogonal (i.e., $\|W^{T}W-I_{k}\|^{2}$ is small). However in practice, enforcing the orthogonality between the base at every iteration is a constraint too strong to decompose audio signal~\cite{laroche2015structured}. 

The sparsity of the dictionary matrix is an interesting property for the decomposition of audio signals and especially for the decomposition of harmonic instruments with very localized harmonic spectra. Contrary to the NMF, the sparsity of PNMF and is an inherent features of the decomposition. These key properties of PNMF motivated us to decompose the harmonic instruments with the orthogonal basis functions.


\subsection{Principle of the SPNMF}

The orthogonal basis functions of PNMF are not flexible enough to decompose a complex audio signal. As stated in~\cite{canadas2014percussive}, harmonic instruments have sparse basis functions whereas percussive instruments have much flatter spectra. As the columns of $W$ are orthogonal, when two sources overlap in the Time-Frequency (TF) plane only one basis function will represent the mixture which is not adequate for efficient separation. To overcome this problem, we propose to add a standard NMF decomposition term to the PNMF. We can expect that most of the harmonic components will be represented by the orthogonal part while the percussive ones will be the regular NMF components. Using a similar model as in our preliminary work~\cite{laroche2015structured}, let $V$ be the magnitude spectrogram of the input data. The model is then given by
\begin{equation} \label{Cfunction}
V \approx \tilde{V}= V_H + V_{P},
\end{equation}
with $V_P$ the spectrogram of the percussive part and $V_H$ the spectrogram of the harmonic part. $V_H$ is approximated by the PNMF decomposition while $W_P$ is decomposed by NMF components as :
\begin{equation}
V \approx \tilde{V}= W_{H}W_{H}^{T}V + W_{P} H_{P}.
\end{equation}
The data matrix is approximated by an almost orthogonal sparse part that codes the harmonic instruments $V_H = W_HW_H^T V$ and a non constrained NMF part that codes the percussive instruments $V_P = W_PH_P$. We use here a fixed drum dictionary $W_p$ in the percussive part of the SPNMF as a fully unsupervised SPNMF model does not allow for a satisfying harmonic/percussive source separation~\cite{laroche2015structured}.


\subsection{Algorithm Optimization}

In order to obtain such a decomposition, we can use a measure of fit $D(x|y)$ between the data matrix $V$ and the estimated matrix $\tilde{V}$. $D(x|y)$ is a scalar cost function and in this article, we use the Itakura Saito (IS) divergence.



The SPNMF model gives the cost function : 
\begin{equation}\label{InitCost}
\min_{W_H,W_P,H_P \geq 0} D(V|W_{H}W_{H}^{T}V + W_{P} H_{P})  
\end{equation}

A solution of this problem can be obtained by iterative multiplicative update rules following the same strategy as in~\cite{yuanOja2005,Lee01algorithmsfor} which consists in splitting the gradient with respect to (wrt) one variable (here $W_H$ for exemple) $\nabla_{W_H} D(V|\tilde{V})$ in its positive $[\nabla_{W_H} D(V|\tilde{V})]^{+}$ and negative parts $[\nabla_{W_H} D(V|\tilde{V})]^{-}$.
The multiplicative updates for SPNMF are then given by: 
$$W_{H} \leftarrow W_{H} \otimes \frac{ [\nabla_{W_H} D(V|\tilde{V})]^{-} }{[\nabla_{W_H} D(V|\tilde{V})]^{+}}, $$
where $\otimes$ is the Hadamard product or element-wise product. The algorithm \ref{AlgoDictionary} gives us the SPNMF optimization process.
 
\begin{algorithm}[h]
 Input: $V \in \mathbb{R}_{+}^{m \times n} $
 Output: $W \in \mathbb{R}_{+}^{m \times k}$, $W_{train} \in \mathbb{R}_+^{m \times e}$ and $H \in \mathbb{R}_{+}^{e \times n}$
 Initialization\;
 \While{$i \leq$ number of iterations}{
	$H_{P} \leftarrow H_{P} \otimes \frac{ [\nabla_{H_P} D(V|\tilde{V})]^{-} }{[\nabla_{H_P} D(V|\tilde{V})]^{+}}$
	

	$W_{H} \leftarrow W_{H} \otimes \frac{ [\nabla_{W_H} D(V|\tilde{V})]^{-} }{[\nabla_{W_H} D(V|\tilde{V})]^{+}}$
	 \vspace{0.2cm}
	 	
	$i=i+1$ 
 }
 $ X_P = W_{train}H_P $ and
 $ X_H = W_HW_H^TV $ 
  
\vspace{0.2cm}
 \caption{SPNMF with the drum dictionary matrix.}\label{AlgoDictionary}
\end{algorithm}




 
\subsection{Signal reconstruction}

The percussive signal $x_p(t)$ is synthesized using the magnitude percussive spectrogram $X_P = W_PH_P$. To reconstruct the phase of the percussive part, we use a generalized Wiener filter~\cite{liutkus2015generalized} to create a percussive mask as:
\begin{equation}
\mathcal{M}_P = \frac{X_P^2}{X_M^2 + X_P^2}.
\end{equation} 
To retrieve the percussive signal as, 
\begin{equation}
x_p(t) = InverseSTFT(\mathcal{M}_P \otimes X).
\end{equation}
Where $X$ is the complex spectrogram of the mixture.
Similarly for the harmonic part, we obtain:
\begin{equation}\label{percuweiner}
\mathcal{M}_H = \frac{X_H^2}{X_M^2 + X_P^2},
\end{equation}
and:
\begin{equation}
x_h(t) = InverseSTFT(\mathcal{M}_H \otimes X).
\end{equation}




\section{Construction of the dictionary}

In this section, we present the test conducted on the SiSec database in order to find the optimal parameter to build the genre specific dictionaries. 

\subsection{Optimal size for the dictionary}\label{optimalsize}

The first step to build a NMF drum dictionary is to select the rank of factorization. We run the optimization tests on the public SiSec database from~\cite{SiSec10} to avoid overtraining our algorithm. It is composed of polyphonic real-world music excerpts and each music signal contains percussive, harmonic instruments and vocals. The duration of the four recording is ranging from $14$ to $24$~s. The goal is to perform an harmonic/percussive decomposition. Following the same protocol as~\cite{canadas2014percussive}, we will not consider the vocal part and we will build the mixture signals from the percussive and harmonic instruments only. All the signals are sampled at $44.1kHz$. We compute the STFT with a and $2048$ sample-long Hann window with a $50\%$ overlap.

The drum signal used for the training comes from the database \cite{gillet2006enst} and the signal is around $3$ min long. We used $14$ files from the database were the drummer is playing a \emph{drum phrase}. We compute an NMF decomposition with different rank of factorization ($k=12$, $k=50$, $k=100$, $k=500$, $k=1000$ and $k=2000$) on the drum signal alone to obtain $6$  drum dictionaries.

The dictionaries are then used to perform a HPSS on the four songs of the SiSEC database using the SPNMF algorithm. The results are compared by means of the Signal-to-Distortion Ratio (SDR), the Signal-to-Interference Ratio (SIR) and the Signal-to-Artifact ratio (SAR) of each of the separated sources using the BSS Eval toolbox provided in \cite{bsseval}.

\begin{figure}[h]

  \centering 
  \includegraphics[width=9cm]{figs/AllDictSizeISMIR.eps}
%  \vspace{2.0cm}
  \caption{\label{dictsize} Average results on the SiSec database.}
  
\end{figure}

The results on the figure \ref{dictsize} show that the optimal value for the SDR and SIR is reached for $k=100$, then the SDR decrease rapidly for $k\geqslant 500$. The high value of SAR (for $k\geqslant 500$) are explained because the separation process is not satisfying. The harmonic signal given at the end of the algorithm is composed of most of the original signal therefore the SAR is very high but the decomposition quality is poor. 

We can conclude that for a $3$ min drum signal, the optimal dictionary size for the SPNMF algorithm is $k=100$.  

\subsection{Database}\label{database}

The dataset is taken from medley-dB \cite{bittner2014medleydb}, it is composed of polyphonic real-world music excerpts. It has $122$ music signals and $87$ of them contain percussive instruments, harmonic instruments and vocals. The signals that do not contain a percussive part are not part of the evaluation. We will be using the song of the genre, \emph{Classical} ($8$ songs), \emph{Singer/Songwriter} ($17$ songs), \emph{Pop} ($10$ songs), \emph{Rock} ($20$ songs), \emph{Jazz} ($11$ songs), \emph{Electronic/Fusion} ($13$ songs) and \emph{World/Folk} ($6$ songs). Because the notion of genre is quite subjective (see Section \ref{defgenre}), the medley-dB database uses general genre labels. These labels should not be considered to be "precise" genre labels. There are many instances where a song could have fallen in multiple genres, and the choices were made so that each genre would be as acoustically homogeneous as possible. As we are only working with the instrumental part of the song with the \emph{Pop} label (for example) are similar to the \emph{Singer/Songwriter}.
We use $3$ song of each genre to use as a training database. The songs used for the training part are not part of the evaluation. To compare the genre specific dictionary, we build a non specific/universal dictionary built using one half of one song of each genre. The files selected are:


\begin{table} 
	\centering 
	\small
   \begin{tabular}{|l|l|}
   \hline
   Genre & Artist Song \\
   	\hline   
Classical  & JoelHelander Definition \\
 & MatthewEntwistle AnEveningWithOliver \\
 & MusicDelta Beethoven \\
\hline
Electronic/Fusion & EthanHein 1930sSynthAndUprightBass \\
 & TablaBreakbeatScience Animoog \\
 & TablaBreakbeatScience Scorpio \\
\hline
Jazz & CroqueMadame Oil \\
 & MusicDelta BebopJazz \\
 & MusicDelta ModalJazz \\
\hline
Pop &  DreamersOfTheGhetto HeavyLove \\
 & NightPanther Fire \\
 & StrandOfOaks Spacestation \\
\hline
Rock & BigTroubles Phantom \\
 & Meaxic TakeAStep \\
 & PurlingHiss Lolita \\
\hline
Singer/Songwriter	& AimeeNorwich Child \\
 & ClaraBerryAndWooldog Boys \\
 & InvisibleFamiliars DisturbingWildlife \\
\hline
World/Folk & AimeeNorwich Flying \\
 &KarimDouaidy Hopscotch \\
 & MusicDelta ChineseYaoZu\\
\hline
Non specific & JoelHelander Definition \\
 & TablaBreakbeatScience Animoog \\
 & MusicDelta BebopJazz \\
 & DreamersOfTheGhetto HeavyLove \\
 &  BigTroubles Phantom \\
 & AimeeNorwich Flying \\
 &  MusicDelta ChineseYaoZu\\
 \hline

  
\end{tabular} 
\caption{\label{trainingdata} Song selected for the training database.}

\end{table}





\subsection{Genre specific dictionaries}\label{genrespecdict}

The NMF model is:
\begin{equation}
V \approx \tilde{V} = WH.
\end{equation}
If $V$ is the power spectrum of a drum signal, The matrix $W$ is a {\em dictionary} or a set of {\em patterns} that codes the frequency information of the drum. Building a dictionary specific to an instrument that performs well on a large database is a complicated problem. Here we build genre specific drum dictionary using the medley-dB database. Using dictionary specific to the genre of music allows us to have dictionaries that a more specific to the signal to decompose.

Table \ref{lengthDict} gives us the length and the energy of the training signal for each genre. \emph{Classical} and \emph{Electronic/Fusion} songs are composed of songs where the drum is only playing for a few moments and even though the length of the signal is higher than the other, the energy is around the same for all the signals. With the results from Section \ref{optimalsize} the dictionary are build has follow. For every genre specific database of the training database, we perform and NMF on the drum signals with $k=300$ (we choose $k=100$ per song for the NMF). Then the dictionaries are used in the SPNMF algorithm as the matrix $W_P$ (see algorithm \ref{AlgoDictionary}).

\begin{table}
   
	\centering 
   \begin{tabular}{|l|l|l|}
\hline   
Genre & Length(min) & Energy\\
\hline
Classical  & 22.06 & 12200 \\
\hline
Electronic/Fusion & 18.66 & 49800\\
\hline
Jazz & 10.96 & 49900\\
\hline
Pop &  12.53 & 39300\\
\hline
Rock & 11.43& 79000\\
\hline
Singer/Songwriter & 9.36 & 48300\\
\hline
World/Folk & 9.53 & 46700\\
\hline
Non Specific (Mix) & 11.03 & 30000 \\
\hline
  
\end{tabular} 
\caption{\label{lengthDict} Length and energy of the genre specific database.}
\end{table}


\section{Results}


\subsection{Comparison of the dictionaries}

In this section we present the results of the algorithm with the genre specific dictionaries on the $66$ song from test database Medley-dB. We perform an HPSS on the audio files using the SPNMF with the $8$ dictionaries created on Section \ref{genrespecdict}. The results on each of the songs are then sorted by genre and the average results are displayed using box-plot. Each box-plot is made up of a central line indicating the median of the data, upper and lower box edges indicating the $1^{st}$ and $3^{rd}$ quartiles while the whiskers indicate the minimum and maximum values. 

The Figure \ref{sdrpop}, \ref{sirpop} and \ref{sarpop} shows the SDR, SAR and SIR results for all the dictionaries on the \emph{Pop} subsection. It gives us a overall idea on how all the dictionaries perform on the same database. The results using the \emph{Pop} dictionary has the highest SDR and SIR results. The non specific dictionary is not performing as well the pop dictionary, it allows us to say that on this \emph{Pop} database, the genre specific dictionary is giving relevant information to the algorithm. As stated in Section \ref{database}, some genre are similar to other and that can explain why the \emph{Rock} and \emph{Singer} dictionaries are giving good results too. 
And interesting result to note is that compared to the non specific dictionary, the genre specific \emph{Pop} dictionary has a lower variance. The genre information allows for a higher robustness to the variety of the songs within the same genre.  

\begin{figure}[h]

  \centering 
  \includegraphics[width=9cm]{figs/SDRismirpop.eps}
%  \vspace{2.0cm}
  \caption{\label{sdrpop} Results on the Pop sub-database using the SPNMF with the 8 dictionaries.}
  
\end{figure}\begin{figure}[h]

  \centering 
  \includegraphics[width=9cm]{figs/SIRismirpop.eps}
%  \vspace{2.0cm}
  \caption{\label{sirpop} Results on the Pop sub-database using the SPNMF with the 8 dictionaries.}
  
\end{figure}\begin{figure}[h]

  \centering 
  \includegraphics[width=9cm]{figs/SARismirpop.eps}
%  \vspace{2.0cm}
  \caption{\label{sarpop} Results on the Pop sub-database using the SPNMF with the 8 dictionaries.}
  
\end{figure}


%
%\begin{table}
%   
%   \begin{tabular}{|l|l|l|}
%\hline   
%Genre & Genre specific Results(dB) & Non specific (dB)\\
%\hline
%Classical  &  &\\
%SDR & 2.95 & \bf{3.00} \\
%SIR & \bf{9.41} & 9.25 \\
%SAR & 12.0 & \bf{17.6}  \\
%\hline
%Electronic/Fusion & &\\
%SDR & \bf{0.54} & 0.50 \\
%SIR & 8.51 & \bf{9.18} \\
%SAR & 11.9 & \bf{14.1} \\
%\hline
%Jazz & &\\
%SDR & 5.94 & \bf{6.02} \\
%SIR & \bf{11.1} & 10.5 \\
%SAR & \bf{15.6} & 15.4 \\
%\hline
%Pop &   &\\
%SDR & \bf{3.76} & 3.40 \\
%SIR & \bf{9.77} & 8.72 \\
%SAR & 13.9 & \bf{14.5} \\
%\hline
%Rock & &\\
%SDR & \bf{0.97} & -0.11  \\
%SIR & \bf{12.0} & 10.6 \\
%SAR & \bf{19.1} & 18.3 \\
%\hline
%Singer/Songwriter & &\\
%SDR & \bf{4.69} & 3.71 \\
%SIR & 9.80 & \bf{10.3} \\
%SAR & \bf{17.2} & 16.9 \\
%\hline
%World/Folk &  &\\
%SDR & \bf{1.83} & 0.66 \\
%SIR & \bf{8.33} & 7.53 \\
%SAR & \bf{18.6} & 16.4 \\
%\hline
%  \end{tabular} 
%\caption{\label{specresults} Mean SDR, SIR and SAR results of the Medley-dB database.}
%\end{table}

\begin{table*}
   
   \begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline   
Genre & Classical & Electronic/Fusion & Jazz & Pop & Rock & Singer/Songwriter & World/Folk \\
\hline
Genre specific Results(dB)  & & & & & & & \\
SDR & 2.95     & \bf{0.54}& 5.94     & \bf{3.76}&\bf{0.97} &\bf{4.69} &\bf{1.83} \\
SIR & \bf{9.41}& 8.51     & \bf{11.1}& \bf{9.77}&\bf{12.0} & 9.80     &\bf{8.33} \\
SAR & 12.0     & 11.9     & \bf{15.6}& 13.9     &\bf{19.1} &\bf{17.2} &\bf{18.6} \\
\hline
Non specific (dB) & & & & & & &\\
SDR & \bf{3.00}& 0.50     & \bf{6.02}& 3.40     & -0.11    &  3.71    & 0.66 \\
SIR & 9.25     & \bf{9.18}& 10.5     & 8.72     & 10.6     &\bf{10.3} & 7.53 \\
SAR & \bf{17.6}& \bf{14.1}& 15.4     & \bf{14.5}& 15.4     & 16.9     & 16.4 \\
\hline
  \end{tabular} 
\caption{\label{specresults} Mean SDR, SIR and SAR results on the Medley-dB database.}
\end{table*}



On Table \ref{specresults}, we display the mean separation score for all the genre specific dictionaries compared to the non specific dictionary. The genre specific dictionaries outperform the universal dictionary by a considerable margin.


\subsection{Discussion}


On the database \emph{Singer/Songwriter}, \emph{Pop}, \emph{Rock}, \emph{Jazz} and \emph{World/Folk}, learning a genre specific dictionary outperform the universal dictionary. The similar pitch content of the music of the same genre is not altered by the NMF decomposition and this information improve the separation.  
The database \emph{Classical} and \emph{Electronic/Fusion} are composed of songs where the drum is only playing for a few moments. Similarly on some songs of the \emph{Electronic/Fusion} database, the electronic drum reproduces the same pattern during the whole song making the drum part very redundant. Because of these two factors, the genre specific dictionaries are not performing correctly.

The main drawback of using a NMF dictionary is that the decomposition is not unique and can any permutation is also a solution. Because of that, we lose the temporal structure of the original drum signal. 



\section{Conclusion}

Using genre specific information in order to build more relevant drum dictionaries is a powerful method to improve the HPSS. The dictionaries still have an imprint of the genre even after the NMF decomposition. 

This is a first step in order to produce dictionaries capable of extracting a wide variety of audio signal.

Future work will be dedicated into building a blind method to select the genre specific dictionary in order to perform the same technique on database were the genre information is not present. 



\section{Annexe}
 
\subsection{Itakura Saito divergence}\label{ISdisteq}
The Itakura Saito divergence gives us the problem,
$$\min_{W_1,W_2,H_2 \geq 0} \frac{V}{\tilde{V}} - log(\frac{V}{\tilde{V}}) +1.$$

The gradient wrt $W_1$ gives
$$[\nabla_{W_1} D(V|\tilde{V})]_{i,j}^{-} = (ZV^TW_1)_{i,j} + (VZ^TW_1)_{i,j},$$
with $Z_{i,j} = (\frac{V}{W_1W_1^TV + W_2H_2})_{i,j}$. 
The positive part of the gradient is
$$[\nabla_{W_1} D(V|\tilde{V})]_{i,j}^{-} = (\phi V^TW_1)_{i,j} + (V \phi^T W_1)_{i,j},$$
with $$ \phi_{i,j} = (\frac{I}{W_1W_1^TV + W_2H_2})_{i,j}.$$ and $I = ones(size(V))$.


Similarly, the gradient wrt $W_2$ gives
$$ [\nabla_{W_2} D(V|\tilde{V})]^{-} = VH_2^T $$
and
$$ [\nabla_{W_2} D(V|\tilde{V})]^{+} = W_1W_1^TVH_2^T + W_2H_2H_2^T.$$

Finally, the gradient wrt $H_2$ gives
$$ [\nabla_{H_2} D(V|\tilde{V})]^{-} = W_2^TV  $$
and
$$ [\nabla_{H_2} D(V|\tilde{V})]^{+} = 2W_2^TW_1W_1^TV + W_2^TW_2H_2. $$




% For bibtex users:
\bibliography{reference}

% For non bibtex users:
%\begin{thebibliography}{citations}
%
%\bibitem {Author:00}
%E. Author.
%``The Title of the Conference Paper,''
%{\it Proceedings of the International Symposium
%on Music Information Retrieval}, pp.~000--111, 2000.
%
%\bibitem{Someone:10}
%A. Someone, B. Someone, and C. Someone.
%``The Title of the Journal Paper,''
%{\it Journal of New Music Research},
%Vol.~A, No.~B, pp.~111--222, 2010.
%
%\bibitem{Someone:04} X. Someone and Y. Someone. {\it Title of the Book},
%    Editorial Acme, Porto, 2012.
%
%\end{thebibliography}

\end{document}
